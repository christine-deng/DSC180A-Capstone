{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde55aab",
   "metadata": {},
   "source": [
    "1. One factor that is essential for consideration during AI model development and performance is the process of data collection and quality assurance. It is important for data to be representative of the whole population and spectrum of diversity for whoever the target audience of the algorithm is, as models that are trained on skewed or incomplete data will have inherent biases that can have severely detrimental consequences for certain populations of people. For example, a model trained on historical data may inadvertently reflect historical racial biases and prejudices. Furthermore, data should be preprocessed in a way that does not impact the meaning of the data through methods like imputation and outlier detection and removal. \n",
    "    Another factor that is essential for consideration during model development is the process of feature selection and engineering, because the features chosen in the final model have a significant impact on its performance and fairness. Certain features may correlate in a biased way with demographic factors such as race and sex, even if it doesn’t appear so on the surface; therefore, it is vital to thoroughly examine all features and their correlations during EDA and select features that are crucial to solving the problem without introducing biases and prejudices.\n",
    "    A third factor that is essential for consideration during model performance is the deployment stage, in which the model is in the real-world making predictions. It is essential for models to be constantly monitored for performance and fairness and for metrics like disparate impact to be measured as the model continues to work, and as more data is added and the model is trained and improved, for these metrics to continue to be evaluated. As society changes, the models need to be updated accordingly to ensure fairness. \n",
    "\n",
    "\n",
    "2. AI has a significant impact in real-world applications by influencing decision-making and interacting with various stakeholders. To measure this impact and audit for bias and clinical impact, there are several key factors that must be considered. Fairness metrics and bias assessment tools like AIF360 are important to evaluate fairness across all demographics using numbers like disparate impact, equal opportunity, and equalized odds. The model’s performance should also be evaluated by experts in a given field, such as doctors evaluating a diagnosis algorithms’ performance and the process that went into model development, to better ensure that the model is performing up to high standards. Furthermore, transparency during the entire process of model development and all of the decisions that were made during this process is vital to ensure model reproducibility as well as to help determine if there were decisions made that inadvertently have an impact on the model’s fairness, as third party auditors would be able to detect. Receiving feedback from various stakeholders and users of the algorithm is another way to understand model performance and audit for bias.\n",
    "    \n",
    "3. Disparate impact refers to when an algorithm’s decision making disproportionately impacts certain individuals or groups of people based on certain demographic characteristics, such as race or sex. In examining healthcare and the MEPS dataset specifically, disparities in data collection and model development methods can adversely impact or differently impact different communities of people. If the type of data collected has selection bias, primarily focusing on specific groups of people or excluding certain populations, then the datasets will be biased to misrepresent the entire population. Secondly, the dimension of the data is essential for consideration because we do not want to overlook important factors that can impact health and would be important for the performance of our model. Third, the collection method can be poorly designed; surveys with misleading questions or containing incomplete information could result in misrepresentation of data and poor quality. Finally, biases in the data collected could lead to the stigmatization or reinforced prejudices against groups of people that are no accurately represented. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
